---
title: Train multi-modal LLM to understand diverse speech paralinguistics by distilling
  from teacher with meta-information prompt
abstract: A Large Language Model (LLM) can be extended for input audio, by expressing
  the audio embeddings as an interpretable prompt to the LLM. The adaptor that computes
  these audio embeddings is often trained using multi-modal Instruction Fine-Tuning
  (IFT) data. It is labour intensive to scale up the creation of such data to many
  datasets, tasks, and audio information types. The labour can be reduced with Knowledge
  Distillation (KD), by prompting an external teacher LLM with meta-information from
  the audio dataset, and using the teacherâ€™s output as a reference to train a student
  that is now prompted with the audio. Prior KD work has only used a few datasets,
  and does not present experiment comparisons against fair choices of IFT models.
  This paper scales up KD to a larger collection of datasets, comprising a wider variety
  of meta-information types. Fair experiment comparisons on Dynamic-SUPERB and AudioBench
  show that KD and IFT are complementary, but KD alone may not outperform IFT.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wong26a
month: 0
tex_title: Train multi-modal {LLM} to understand diverse speech paralinguistics by
  distilling from teacher with meta-information prompt
firstpage: 61
lastpage: 77
page: 61-77
order: 61
cycles: false
bibtex_author: Wong, Jeremy and Huzaifah, Muhammad and Sailor, Hardik and Sun, Shuo
  and Tan, Kye Min and Wang, Bin and Wang, Qiongqiong and Zhang, Wenyu and Zou, Xunlong
  and Chen, Nancy F. and Aw, Ai Ti
author:
- given: Jeremy
  family: Wong
- given: Muhammad
  family: Huzaifah
- given: Hardik
  family: Sailor
- given: Shuo
  family: Sun
- given: Kye Min
  family: Tan
- given: Bin
  family: Wang
- given: Qiongqiong
  family: Wang
- given: Wenyu
  family: Zhang
- given: Xunlong
  family: Zou
- given: Nancy F.
  family: Chen
- given: Ai Ti
  family: Aw
date: 2026-02-27
address:
container-title: 'Proceedings of the AAAI 2026 Workshop on Audio-Centric AI: Towards
  Real-World Multimodal Reasoning and Application Use Cases (Audio-AAAI)'
volume: '312'
genre: inproceedings
issued:
  date-parts:
  - 2026
  - 2
  - 27
pdf: https://raw.githubusercontent.com/mlresearch/v312/main/assets/wong26a/wong26a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
