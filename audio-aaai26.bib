@Proceedings{AUDIO-AAAI2026,
 booktitle = {Proceedings of the {AAAI} 2026 Workshop on Audio-Centric {AI}: Towards Real-World Multimodal Reasoning and Application Use Cases ({Audio-AAAI})},
 name = {Audio-Centric {AI}: Towards Real-World Multimodal Reasoning and Application Use Cases ({Audio-AAAI})},
 shortname = {Audio-AAAI},
 year = {2026},
 editor = {Komatsu, Tatsuya and Imoto, Keisuke and Gao, Xiaoxue and Ono, Nobutaka and Chen, Nancy F.},
 volume = {312},
 start = {2026-01-26},
 end = {2026-01-26},
 address = {Singapore EXPO, Singapore, Singapore},
 conference_url = {https://sites.google.com/view/audio-aaai},
 conference_number = {1},
}

@InProceedings{lemerle26,
 title = {Lina-Speech: Gated Linear Attention and Initial-State Tuning for Multi-Sample Prompting Text-To-Speech Synthesis},
 author = {Lemerle, Théodor and Guichoux, Téo and Roebel, Axel and Obin, Nicolas},
 pages = {},
 abstract = {Neural codec language models, built on transformer architecture, have revolutionized text-to-speech (TTS) synthesis, excelling in voice cloning by treating it as a prefix continuation task. However, their limited context length limits their effectiveness to short speech samples. As a result, the voice cloning ability is restricted to a limited coverage and diversity of the speaker's prosody and style. Besides, adapting prosody, accent, or appropriate emotion from a short prefix remains a challenging task. Finally, the quadratic complexity of self-attention limits inference throughput. In this work, we introduce Lina-Speech, a TTS model with Gated Linear Attention (GLA) to replace standard self-attention as a principled backbone, improving inference throughput while matching state-of-the-art performance. Leveraging the stateful property of recurrent architecture, we introduce an Initial-State Tuning (IST) strategy that unlocks the possibility of multiple speech sample conditioning of arbitrary numbers and lengths and provides a comprehensive and efficient strategy for voice cloning and out-of-domain speaking style and emotion adaptation. We demonstrate the effectiveness of this approach for controlling fine-grained characteristics such as prosody and emotion. We will release our code and checkpoints. We encourage the reader to listen to the audio samples on our demo page: https://anonymoussbm.github.io/aaai-audio-workshop.github/},
}

@InProceedings{kishi26,
 title = {AudioBERTScore: Objective Evaluation of Environmental Sound Synthesis Based on Similarity of Audio Embedding Sequences},
 author = {Kishi, Minoru and Sakai, Ryosuke and Takamichi, Shinnosuke and Kanamori, Yusuke and Okamoto, Yuki},
 pages = {},
 abstract = {We propose a novel objective evaluation metric for synthesized audio in text-to-audio generation (TTA), aiming to improve the performance of TTA models. In TTA, subjective evaluation of synthesized sounds is important; however, conducting it requires significant monetary and time costs. Therefore, objective evaluation such as mel-cepstral distortion are used, but the correlation between these objective metrics and subjective evaluation values is weak. Our proposed objective evaluation metric, AudioBERTScore, calculates the similarity between embedding of the synthesized and reference sounds. The method is based not only on the max-norm used in conventional BERTScore but also on the p-norm to reflect the non-local nature of environmental sounds. Experimental results show that scores obtained by the proposed method have a higher correlation with subjective evaluation values than conventional metrics.},
}

@InProceedings{li26,
 title = {Semi-supervised Acoustic Scene Classification under Spatial-Temporal Variability with a {CRNN}-based Model},
 author = {Li, Haowen and Wang, Mou and Luo, Zhengding and Tan, Ee-Leng and Yang, Ziyi and Gan, Woon-Seng},
 pages = {},
 abstract = {In this work, we present MobileASCNet, a lightweight CRNN-based model designed for acoustic scene classification (ASC) under spatial-temporal variability, as defined in the APSIPA ASC 2025 Grand Challenge. The model combines depthwise separable convolutions and ResNet-inspired residual blocks for efficient spatial feature extraction, and employs a gated recurrent unit (GRU) branch to capture temporal dependencies. City and time embeddings are fused to enhance context-awareness. We conduct extensive comparisons under different training strategies, including training from scratch, pretraining with fine-tuning, and feature freezing. Without relying on knowledge distillation, MobileASCNet achieves a competitive classification accuracy on the development set, with low model complexity.},
}

@InProceedings{nakashima26,
 title = {Online Independent Low-Rank Matrix Analysis as a Lightweight and Trainable Model for Real-Time Multichannel Music Source Separation},
 author = {Nakashima, Taishi and Ono, Nobutaka},
 pages = {},
 abstract = {In this paper, we propose an online extension of independent low-rank matrix analysis (ILRMA) for blind music source separation under real-time constraints. Because multitrack stems are rarely released, we target lightweight processing that operates directly on in-the-wild mixtures. The method combines an online Itakura--Saito nonnegative matrix factorization (NMF) update with an online auxiliary-function independent vector analysis (IVA) framework, preserving the low-rank spectral model employed in ILRMA while updating the demixing matrix frame by frame with bounded latency and memory. Simulations on multitrack music mixtures show improved separation accuracy and a real-time factor below one, indicating feasibility for live and interactive scenarios. These results suggest blind separation suitable for low-latency music applications.},
}

@InProceedings{wong26,
 title = {Train multi-modal {LLM} to understand diverse speech paralinguistics by distilling from teacher with meta-information prompt},
 author = {Wong, Jeremy and Huzaifah, Muhammad and Sailor, Hardik and Sun, Shuo and Tan, Kye Min and Wang, Bin and Wang, Qiongqiong and Zhang, Wenyu and Zou, Xunlong and Chen, Nancy F. and Aw, Ai Ti},
 pages = {},
 abstract = {A Large Language Model (LLM) can be extended for input audio, by expressing the audio embeddings as an interpretable prompt to the LLM. The adaptor that computes these audio embeddings is often trained using multi-modal Instruction Fine-Tuning (IFT) data. It is labour intensive to scale up the creation of such data to many datasets, tasks, and audio information types. The labour can be reduced with Knowledge Distillation (KD), by prompting an external teacher LLM with meta-information from the audio dataset, and using the teacher's output as a reference to train a student that is now prompted with the audio. Prior KD work has only used a few datasets, and does not present experiment comparisons against fair choices of IFT models. This paper scales up KD to a larger collection of datasets, comprising a wider variety of meta-information types. Fair experiment comparisons on Dynamic-SUPERB and AudioBench show that KD and IFT are complementary, but KD alone may not outperform IFT.},
}

@InProceedings{huzaifah26,
 title = {Latent-{RQ}: Enhancing Speech Pre-training with Latent Representations and Random Quantization},
 author = {Huzaifah, Muhammad and Sailor, Hardik and Wong, Jeremy and Chen, Nancy F. and Aw, Ai Ti},
 pages = {},
 abstract = {Random quantization is a simple yet effective strategy for speech self-supervised pre-training, producing strong encoder representations for a range of downstream tasks. However, existing methods such as BEST-RQ rely on Mel spectrograms -- low-level acoustic features -- as quantizer inputs, which may hinder convergence and limit target quality. We propose Latent-RQ, an extension that replaces direct Mel inputs with richer latent representations extracted from a pre-trained encoder. To further enhance target quality, the target encoder is periodically updated during training. Latent-RQ achieves consistent improvements on the SUPERB benchmark, particularly for speech recognition and speaker identification, while reaching comparable performance to BEST-RQ with fewer optimization steps under a fixed training budget. We also analyze how target layer selection influences downstream performance and layer-wise information encoding. t-SNE visualizations of phoneme and speaker embeddings reveal clearer clustering and improved target discriminability. Overall, Latent-RQ offers a scalable and effective enhancement to random quantization-based SSL frameworks for speech representation learning.},
}

@InProceedings{cacioli26,
 title = {Can You Hear Naples? Building and Benchmarking a Neapolitan Speech Corpus},
 author = {Cacioli, Michael and Eggleston, Liam and Sarabu, Jatin and Zhu, Kevin},
 pages = {},
 abstract = {This paper presents the creation and analysis of the first spoken corpus for Neapolitan, a richly historic but under-resourced Romance dialect of Southern Italy. Despite its cultural importance, Neapolitan has been largely omitted from computational resources, limiting both dialectological research and the development of equitable speech technologies. We address this gap by creating the first structured spoken resource for Neapolitan, enabling systematic evaluation of dialectal ASR performance. Each clip was manually transcribed in orthographic Neapolitan and automatically aligned using OpenAI's Whisper API, configured for standard Italian. To figure out how well Whisper transcribed the spoken Neapolitan sentences, we checked the outputs against the correct human-written texts using a few different methods. Specifically, we looked at how often the words matched (BLEU), how different the transcriptions were overall (normalized Levenshtein distance), and how closely the sets of words lined up (Jaccard similarity). We also used Word Error Rate (WER), but to make it easier to interpret, we converted it to similarity by subtracting from one (1--WER). A higher value means the transcription was more accurate. On average, this similarity measure came out very low, around 0.1306 (σ = 0.1654), meaning roughly 87 percent of the words were transcribed incorrectly. The other evaluation measures told the same story: normalized Levenshtein similarity averaged around 0.6360, and Jaccard similarity was just 0.1078. This paper makes three crucial steps: (1) developed an easy-to-follow process anyone can use to build similar datasets for other dialects, (2) released the first openly accessible Neapolitan speech corpus, and (3) demonstrated just how critical it is to build ASR systems specifically trained on dialects, supporting not just computational linguistic research but also efforts to preserve these unique languages.},
}

@InProceedings{lin26,
 title = {{AudioRAG}: A Challenging Benchmark for Audio Reasoning and Information Retrieval},
 author = {Lin, Jingru and Zhang, Chen and Wang, Tianrui and Li, Haizhou},
 pages = {},
 abstract = {Due to recent advancements in Large Audio-Language Models (LALMs) that demonstrate remarkable performance across a range of sound-, speech- and music-related tasks, there is a growing interest in proposing benchmarks to assess these models. Existing benchmarks generally focus only on reasoning with internal knowledge, neglecting real-world scenarios that require external information grounding. To bridge this gap, we introduce AudioRag, a novel benchmark designed to evaluate audio-based reasoning augmented by information retrieval in realistic web environments. This benchmark comprises both LLM-generated and manually curated question-answer pairs. Our evaluations reveal that even the state-of-the-art LALMs struggle to answer these questions. We therefore propose an agentic pipeline that integrates audio reasoning with retrieval-augmented generation, providing a stronger baseline for future research.},
}

